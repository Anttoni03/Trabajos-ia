{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e990e6801202c40d",
   "metadata": {
    "id": "e990e6801202c40d"
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/bmalcover/aa_2425/blob/main/10_VGG_Custom_Dataset/VGG_Custom.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "272d21b93244ea6",
   "metadata": {
    "id": "272d21b93244ea6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a26d98415d24c4",
   "metadata": {
    "id": "66a26d98415d24c4"
   },
   "source": [
    "# Introducció\n",
    "\n",
    "En aquesta pràctica, treballarem amb un conjunt de dades d'imatges de cans i moixos que ja heu utilitzat a la pràctica de la part anterior de l'assignatura. Tot i que coneixem aquest conjunt de dades, en aquesta ocasió ho em d'emprar amb ``Pytorch`` per tant haurem d'adaptar com ho llegim: fent un pre-processat de les dades per aplicar-los nous models d'aprenentatge profund.\n",
    "\n",
    "L'objectiu de la sessió serà experimentar amb les diferents versions de la xarxa de convolució ``VGG``, com ara VGG16 i VGG19. Aquesta pràctica us permetrà veure com canvien els resultats d'entrenament i predicció en funció de l'arquitectura emprada, i reflexionar sobre els avantatges i inconvenients d'incrementar la profunditat d'una xarxa de convolució.\n",
    "\n",
    "# Preparam les dades\n",
    "Primerament descarregam les dades en el Google Colab. Per tal de fer-ho emprarem les eines ``wget`` i ``unzip``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8ca53b-abc7-4835-ab76-b80c3b51a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/bmalcover/aa_2425/releases/download/v1/gatigos.zip\n",
    "!unzip gatigos.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dcf25d5b66ce14",
   "metadata": {
    "id": "78dcf25d5b66ce14"
   },
   "source": [
    "# *Custom dataset*\n",
    "\n",
    "Un dataset personalitzat en PyTorch permet preparar i gestionar dades específiques per a un projecte d’aprenentatge automàtic. La classe ``Dataset`` de ``PyTorch`` és la base per crear aquest tipus de dataset i requereix la implementació de tres mètodes essencials: ``__init__``, ``__len__``, i ``__getitem__``.\n",
    "\n",
    "* ``__init__``: Aquest mètode inicialitza el ``dataset`` i defineix els paràmetres que seran necessaris, com ara la ubicació de les dades o qualsevol transformació a aplicar. Aquí es poden carregar rutes d'imatges o etiquetes i definir les transformacions que es realitzaran.\n",
    "\n",
    "* ``__len__``: Aquest mètode retorna el nombre total d'exemples en el ``dataset``. ``PyTorch`` l'utilitza per saber quantes mostres conté el conjunt de dades, cosa que és essencial per crear les batchs d’entrenament.\n",
    "* ``__getitem__``: Aquest mètode accedeix a una mostra concreta del ``dataset``, identificada per un índex, i retorna les dades i la seva etiqueta (o ``target``). Normalment, s’apliquen les transformacions aquí abans de retornar la mostra, per assegurar que cada dada té el format adequat per al model.\n",
    "\n",
    "Un cop creada, aquesta classe es pot emprar amb el ``DataLoader`` de ``PyTorch`` per gestionar l'entrenament en *batchs*, fent que el dataset personalitzat sigui eficient i fàcil de treballar dins del flux d’aprenentatge automàtic de ``PyTorch``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c35b3d5385843893",
   "metadata": {
    "id": "c35b3d5385843893"
   },
   "outputs": [],
   "source": [
    "class CatIGosDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, img_paths, annotations, transform=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            path:\n",
    "            path_anotation:\n",
    "            transform:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.img_paths = img_paths\n",
    "        self.annotations = annotations\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.img_paths[idx])\n",
    "        label = self.annotations[self.img_paths[idx]]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd3915f6ecb80489",
   "metadata": {
    "id": "dd3915f6ecb80489"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "paths_imgs = glob.glob(\"images/*.png\")\n",
    "labels = {}\n",
    "\n",
    "for path_img in paths_imgs:\n",
    "    _, name = os.path.split(path_img)\n",
    "    name = name.split(\".\")[0]\n",
    "\n",
    "    name_xml = f\"annotations/{name}.xml\"\n",
    "    tree = ET.parse(name_xml)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    annotation = root.find('object').find('name').text\n",
    "    annotation = 0 if annotation == 'cat' else 1\n",
    "\n",
    "    labels[path_img] = annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a26fdde567a454b",
   "metadata": {
    "id": "8a26fdde567a454b"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(paths_imgs, random_state=42, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f040a135483ff08e",
   "metadata": {
    "id": "f040a135483ff08e"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 5\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_ds = CatIGosDataset(train, labels, transform=transform)\n",
    "test_ds = CatIGosDataset(test, labels, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0756ea88b51da9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0756ea88b51da9e",
    "outputId": "7c47cef6-0d78-4a7a-e5cd-65dec9025fe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 256, 256]) tensor([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "img, target = next(iter(train_loader))\n",
    "print(img.shape, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe645c3e13180bbe",
   "metadata": {
    "id": "fe645c3e13180bbe"
   },
   "source": [
    "# Definicio de la xarxa: VGG i *Transfer learning*\n",
    "\n",
    "En aquesta pràctica aplicarem la tècnica de transfer learning amb una de les xarxes CNN més conegudes i profundes:\n",
    "\n",
    " - VGG. [Very Deep Convolutional Networks for Large-Scale Image Recognition, 2014](https://arxiv.org/abs/1409.1556). La mida d'entrada de les imatges és de (224x224x3). VGG es presenta en diferents variants, com ara VGG16 i VGG19, que contenen respectivament 16 i 19 capes amb aproximadament 138 milions de paràmetres entrenables en el cas de VGG16.\n",
    "\n",
    "Descarregarem VGG i l'analitzarem. En aquest cas, no només obtenim la seva arquitectura, sinó també els pesos resultants del seu entrenament en grans conjunts de dades.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55fbbcc900043cba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55fbbcc900043cba",
    "outputId": "23d41ee7-d152-4526-ae8f-638aa51542c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Toni\\miniconda3\\envs\\iaToni2024\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Arquitectura VGG11\n",
      "--------------------------------------------------\n",
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vgg11 = models.vgg11(weights=True)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Arquitectura VGG11\")\n",
    "print(\"-\" * 50)\n",
    "print(vgg11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fbc1cbf5c836a",
   "metadata": {
    "id": "e1fbc1cbf5c836a"
   },
   "source": [
    "## Com emprar la GPU per entrenar un model\n",
    "\n",
    "Un dels elements diferencials d'aquest model, respecte als que havíem vist fins ara, és la seva mida i, per tant, l'entrenament es torna impossible emprant __CPU__ directament. Per resoldre-ho hem d'emprar una **GPU**, a Google Colab disposam d'elles gratuïtament. Per fer-ho amb *Pytorch* hem de fer tres passes:\n",
    "\n",
    "1. Comprovar que hi ha una GPU disponible.\n",
    "2. Moure el model a GPU.\n",
    "3. Moure les dades a GPU.\n",
    "\n",
    "### Comprova si tenim una GPU disponible\n",
    "\n",
    "Primer de tot, cal verificar si hi ha una GPU disponible a l’entorn. Això es pot fer amb el següent codi:\n",
    "\n",
    "```python\n",
    "\n",
    "import torch\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "```\n",
    "\n",
    "Si la variable ``is_cuda`` és certa, llavors tens accés a una GPU.\n",
    "\n",
    "### Mou el model a la GPU\n",
    "\n",
    "En PyTorch, els models han d'estar explícitament en la GPU per poder fer servir la seva potència de càlcul. Si estàs carregant un model preentrenat (com AlexNet, ResNet, etc.), o si has definit el teu propi model, pots moure’l a la GPU amb ``.to(device)``, on device fa referència a la GPU.\n",
    "\n",
    "```python\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "```\n",
    "\n",
    "Això mou el model a la GPU (si està disponible). Si només tens una CPU, el model es mantindrà a la CPU.\n",
    "\n",
    "### Mou les dades a la GPU\n",
    "\n",
    "No només el model, sinó que també les dades (inputs) han d'estar a la GPU per fer les operacions més ràpides. Així, abans de fer servir les dades com a inputs del model, assegura't de moure-les al mateix device:\n",
    "\n",
    "```python\n",
    "\n",
    "# Exemple d'un batch de dades\n",
    "inputs, labels = inputs.to(device), labels.to(device)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cc3f9f3c6064fa8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3cc3f9f3c6064fa8",
    "outputId": "c278d445-e3c1-4209-e9cf-d98524ec467b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vgg11.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee807ab8ee2cdf1",
   "metadata": {
    "id": "5ee807ab8ee2cdf1"
   },
   "source": [
    "\n",
    "## Feina a fer:\n",
    "\n",
    "1. Preparar el *dataset* personalitzat.\n",
    "2. Carregar la xarxa VGG11, VGG16 i VGG19, amb i sense batch normalization.\n",
    "2. Entrenar-ho fent *transfer learning*.\n",
    "4. Comparar els resultats.\n",
    "\n",
    "**Nota**. Com veureu no us donam aquesta vegada el bucle d'entrenament, sigui com sigui podeu adaptar el vist a les sessions anteriors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41cae526ac948721",
   "metadata": {
    "id": "41cae526ac948721"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "vgg11.classifier = nn.Sequential(\n",
    "    torch.nn.Linear(25088, 1024),\n",
    "    nn.ReLU(),\n",
    "    torch.nn.Linear(1024, 1024),\n",
    "    nn.ReLU(),\n",
    "    torch.nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    torch.nn.Linear(512, 1),  # Ja que tenim 2 classes.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fe4dddcb7238877",
   "metadata": {
    "id": "3fe4dddcb7238877"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "learning_rate = 1e-3  # Hiperparàmetre\n",
    "optimizer = optim.Adam(vgg11.parameters(), lr=learning_rate)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = vgg11.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47d511dc2607fac6",
   "metadata": {
    "id": "47d511dc2607fac6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca836dc71ce642ce85e98f9baa139617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Èpoques:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4669af16cf814a549ac38bf1e6aee253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches (Època 1): 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([4])) must be the same as input size (torch.Size([4, 1]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_pred\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 61\u001b[0m batch_test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (loss_fn(y_pred, y\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[0;32m     63\u001b[0m y_pred_class \u001b[38;5;241m=\u001b[39m (y_pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mdouble()\n\u001b[0;32m     64\u001b[0m batch_test_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m accuracy_score(y, y_pred_class)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\iaToni2024\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\iaToni2024\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\iaToni2024\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:819\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    821\u001b[0m         target,\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    823\u001b[0m         pos_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_weight,\n\u001b[0;32m    824\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m    825\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\iaToni2024\\Lib\\site-packages\\torch\\nn\\functional.py:3624\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m-> 3624\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3625\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3626\u001b[0m     )\n\u001b[0;32m   3628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\n\u001b[0;32m   3629\u001b[0m     \u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum\n\u001b[0;32m   3630\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Target size (torch.Size([4])) must be the same as input size (torch.Size([4, 1]))"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "running_loss = []\n",
    "running_acc = []\n",
    "\n",
    "running_test_loss = []\n",
    "running_test_acc_cnn = []\n",
    "\n",
    "for t in tqdm(range(EPOCHS), desc=\"Èpoques\"):\n",
    "    batch_loss = 0\n",
    "    batch_acc = 0\n",
    "\n",
    "    i_batch = 1\n",
    "    # Iteram els batches.\n",
    "    for i_batch, (x, y) in tqdm(enumerate(train_loader), desc=f\"Batches (Època {t + 1})\"):\n",
    "        vgg11.train()  # Posam el model a mode entranament.\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1. PREDICCIÓ\n",
    "        y_pred = vgg11(x.to(device))\n",
    "\n",
    "        # 2. CALCUL DE LA PÈRDUA\n",
    "        # Computa la pèrdua: l'error de predicció vs el valor correcte\n",
    "        # Es guarda la pèrdua en un array per futures visualitzacions\n",
    "\n",
    "        y = y.to(device)\n",
    "        y = torch.unsqueeze(y, 1)\n",
    "\n",
    "        loss = loss_fn(y_pred, y.float())\n",
    "\n",
    "        #3. GRADIENT\n",
    "        vgg11.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Actualitza els pesos utilitzant l'algorisme d'actualització\n",
    "        #4. OPTIMITZACIO\n",
    "        with torch.no_grad():\n",
    "            optimizer.step()\n",
    "\n",
    "        # 5. EVALUAM EL MODEL\n",
    "        vgg11.eval()  # Mode avaluació de la xarxa\n",
    "\n",
    "        y_pred = vgg11(x.to(device))\n",
    "        batch_loss += (loss_fn(y_pred, y.float()).detach())\n",
    "\n",
    "        y_pred_class = (y_pred.detach().cpu().numpy() > 0.5)\n",
    "        batch_acc += accuracy_score(y.detach().cpu().numpy() , y_pred_class)\n",
    "\n",
    "    running_loss.append(batch_loss / (i_batch + 1))\n",
    "    running_acc.append(batch_acc / (i_batch + 1))\n",
    "\n",
    "    batch_test_loss = 0\n",
    "    batch_test_acc = 0\n",
    "\n",
    "    vgg11.eval()\n",
    "    for i_batch, (x, y) in enumerate(test_loader):\n",
    "        y_pred = vgg11(x.to(device))\n",
    "        batch_test_loss += (loss_fn(y_pred.squeeze(), y.to(device)).detach())\n",
    "\n",
    "        y_pred_class = (y_pred > 0.5).double()\n",
    "        batch_test_acc += accuracy_score(y, y_pred_class)\n",
    "\n",
    "    running_test_loss.append(batch_test_loss / (i_batch + 1))\n",
    "    running_test_acc_cnn.append(batch_test_acc / (i_batch + 1))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
